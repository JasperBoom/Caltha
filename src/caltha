#!/usr/bin/env python3

# Copyright (C) 2018 Jasper Boom

# This program is free software: you can redistribute it and/or modify
# it under the terms of the GNU Affero General Public License version 3 as
# published by the Free Software Foundation.

# This program is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the
# GNU Affero General Public License for more details.

# You should have received a copy of the GNU Affero General Public License
# along with this program. If not, see <https://www.gnu.org/licenses/>.

# Imports
import argparse
import ntpath
import os
import pyfastx
import random
import re
import shutil
import sre_yield
import string
import time
import zipfile
import jellyfish.cjellyfish as cjellyfish
import multiprocessing as mp
import pandas as pd
import subprocess as sp
from Bio.Seq import Seq


# The setOutputFiles function.
# This function creates the tabular and blast output files.
def setOutputFiles(directory_path, output_file_name):
    dfOutput = pd.DataFrame(columns=["UMI_ID", "Read_Count"])
    intCount = 0
    output_blast_file = output_file_name + "_BLAST.fasta"
    output_table = output_file_name + "_TABULAR.tbl"
    for file_name in os.listdir(directory_path + "/post_vsearch_clustering"):
        strUmiNumber = file_name.split("_")[0]
        intLineCount = 0
        with open(directory_path + "/post_vsearch_clustering/" + file_name) as oisClusterFile:
            for strLine in oisClusterFile:
                intLineCount += 1
        with open(directory_path + "/post_vsearch_clustering/" + file_name) as oisUmiFile:
            if intLineCount == 2:
                for strLine in oisUmiFile:
                    if strLine.startswith(">"):
                        strHeader = strLine.split("=")[1].strip("\n")
                        read = next(oisUmiFile)
                        dfOutput.loc[intCount] = [
                            strUmiNumber,
                            strHeader.strip("\n"),
                        ]
                        with open(output_blast_file, "a") as flOutput:
                            flOutput.write(">" + strUmiNumber + "\n")
                            flOutput.write(read.strip("\n").upper() + "\n")
                    else:
                        pass
            elif intLineCount > 2:
                intVersionCount = 1
                for strLine in oisUmiFile:
                    if strLine.startswith(">"):
                        strHeader = strLine.split("=")[1].strip("\n")
                        read = next(oisUmiFile)
                        strUmiVersion = (
                            strUmiNumber + "." + str(intVersionCount)
                        )
                        dfOutput.loc[intCount] = [
                            strUmiVersion,
                            strHeader.strip("\n"),
                        ]
                        with open(output_blast_file, "a") as flOutput:
                            flOutput.write(">" + strUmiVersion + "\n")
                            flOutput.write(read.strip("\n").upper() + "\n")
                        intVersionCount += 1
                        intCount += 1
                    else:
                        pass
            else:
                pass
        intCount += 1
    dfOutput = dfOutput.set_index("UMI_ID")
    dfOutput.to_csv(output_table, sep="\t", encoding="utf-8")


# The setClusterSize function.
# This function controls the Vsearch clustering. Every fasta file created by
# getSortBySize is clustered using Vsearch. The expected result is a single
# centroid sequence. This is checked in the setOutputFiles function.
def setClusterSize(directory_path, identity_percentage):
    for file_name in os.listdir(directory_path + "/vsearch_clustering"):
        if file_name.startswith("sorted"):
            strInputCommand = directory_path + "/vsearch_clustering/" + file_name
            strOutputCommand = directory_path + "/post_vsearch_clustering/" + file_name[11:]
            rafClustering = sp.Popen(
                [
                    "vsearch",
                    "--cluster_size",
                    strInputCommand,
                    "--fasta_width",
                    "0",
                    "--id",
                    identity_percentage,
                    "--sizein",
                    "--minseqlength",
                    "1",
                    "--centroids",
                    strOutputCommand,
                    "--sizeout",
                ],
                stdout=sp.PIPE,
                stderr=sp.PIPE,
            )
            strOut, strError = rafClustering.communicate()
        else:
            pass


# The getSortBySize function.
# This function controls the Vsearch sorting. Every fasta file created by
# getDereplication is sorted based on abundance. Any reads with a abundance
# lower than minimum_abundance will be discarded.
def getSortBySize(directory_path, minimum_abundance):
    for file_name in os.listdir(directory_path + "/vsearch_clustering"):
        if file_name.startswith("derep"):
            strInputCommand = directory_path + "/vsearch_clustering/" + file_name
            strOutputCommand = directory_path + "/vsearch_clustering/" + "sorted" + file_name
            rafSort = sp.Popen(
                [
                    "vsearch",
                    "--sortbysize",
                    strInputCommand,
                    "--output",
                    strOutputCommand,
                    "--minseqlength",
                    "1",
                    "--minsize",
                    minimum_abundance,
                ],
                stdout=sp.PIPE,
                stderr=sp.PIPE,
            )
            strOut, strError = rafSort.communicate()
        else:
            pass


# The getDereplication function.
# This function controls the Vsearch dereplication. Every fasta file created by
# getFastaFile is dereplicated. This step is necessary for the sorting step to
# work.
def getDereplication(directory_path):
    for file_name in os.listdir(directory_path + "/pre_validation"):
        if file_name.endswith(".fasta"):
            strInputCommand = directory_path + "/pre_validation/" + file_name
            strOutputCommand = directory_path + "/vsearch_clustering/" + "derep" + file_name
            rafDerep = sp.Popen(
                [
                    "vsearch",
                    "--derep_fulllength",
                    strInputCommand,
                    "--output",
                    strOutputCommand,
                    "--minseqlength",
                    "1",
                    "--sizeout",
                ],
                stdout=sp.PIPE,
                stderr=sp.PIPE,
            )
            strOut, strError = rafDerep.communicate()
        else:
            pass


# The getFastaFile function.
# This function creates separate fasta files for every unique UMI. The function
# creates a unique name for every UMI file and combines that with the desired
# output path. A file is opened or created based on this combination. The
# read header and the read itself are appended to it.
def getFastaFile(directory_path, unique_umi_dictionary, strHeader, read, strCode):
    fileIdentifier = (
        "UMI#" + str(unique_umi_dictionary[strCode]) + "_" + strCode + ".fasta"
    )
    file_name = directory_path + "/pre_validation/" + fileIdentifier
    with open(file_name, "a") as flOutput:
        flOutput.write(">" + strHeader + "\n")
        flOutput.write(read + "\n")


# The useZeroPosition function.
# This function will isolate either a 5'-end, 3'-end or double UMI based on
# the start or end position of a sequence.
# It will check if both the forward and reverse primer can be found. If this
# check is passed, the 5'-end, 3'-end UMI or double UMI will be isolated by
# adding or subtracting the UMI length from the first or last position of the
# sequence. The function will return (if possible) the UMI nucleotides.
def useZeroPosition(
    umi_location, umi_length, read, forward_anchor, reverse_anchor
):
    tplCheckForward = re.search(forward_anchor, read)
    if tplCheckForward is not None:
        tplCheckReverse = re.search(reverse_anchor, read)
        if tplCheckReverse is not None:
            if umi_location == "umi5":
                return read[0 : int(umi_length)]
            elif umi_location == "umidouble":
                return (
                    read[0 : int(umi_length)],
                    read[-int(umi_length) :],
                )
            elif umi_location == "umi3":
                return read[-int(umi_length) :]
            else:
                pass
        else:
            pass
    else:
        pass


# The useAdapter function.
# This function searches for a regex string in the provided sequence. It will
# isolate either a 5'-end, 3'-end or double UMI. The isolation is based on
# this sequence structure:
#     ADAPTER(F)-UMI(5')-PRIMER(F)-INSERT-PRIMER(R)-UMI(3')-ADAPTER(R).
# When looking for the 5'-end UMI, the last position of ADAPTER(F) is used,
# when looking for the 3'-end UMI, the first position of ADAPTER(R) is used,
# when looking for the double UMI, both mentioned positions are used.
# These positons plus or minus the UMI length result in the UMI nucleotides.
# In the case of umi5 or umi3, a check needs to be passed. This check makes
# sure the opposite adapters are also present, otherwise no UMI is returned.
# The function will return (if possible) the UMI nucleotides.
def useAdapter(umi_location, umi_length, read, forward_anchor, reverse_anchor):
    if umi_location == "umi5" or umi_location == "umidouble":
        intPositionForward = re.search(forward_anchor, read).end()
        intPositionUmiForward = intPositionForward + int(umi_length)
        strUmiForward = read[intPositionForward:intPositionUmiForward]
        if umi_location == "umi5":
            tplCheckReverse = re.search(reverse_anchor, read)
            if tplCheckReverse is not None:
                return strUmiForward
            else:
                pass
        elif umi_location == "umidouble":
            intPositionReverse = re.search(reverse_anchor, read).start()
            intPositionUmiReverse = intPositionReverse - int(umi_length)
            strUmiReverse = read[intPositionUmiReverse:intPositionReverse]
            return strUmiForward, strUmiReverse
        else:
            pass
    elif umi_location == "umi3":
        tplCheckForward = re.search(forward_anchor, read)
        if tplCheckForward is not None:
            intPositionReverse = re.search(reverse_anchor, read).start()
            intPositionUmiReverse = intPositionReverse - int(umi_length)
            strUmiReverse = read[intPositionUmiReverse:intPositionReverse]
            return strUmiReverse
        else:
            pass
    else:
        pass


# The usePrimer function.
# This function searches for a regex string in the provided sequence. It will
# isolate either a 5'-end, 3'-end or double UMI. The isolation is based on
# this sequence structure:
#     UMI(5')-PRIMER(F)-INSERT-PRIMER(R)-UMI(3').
# When looking for the 5'-end UMI, the first position of PRIMER(F) is used,
# when looking for the 3'-end UMI, the last position of PRIMER(R) is used,
# when looking for the double UMI, both mentioned positions are used.
# These positons plus or minus the UMI length result in the UMI nucleotides.
# In the case of umi5 or umi3, a check needs to be passed. This check makes
# sure the opposite primer is also present, otherwise no UMI is returned.
# The function returns (if possible) the UMI nucleotides.
def usePrimer(umi_location, umi_length, read, forward_anchor, reverse_anchor):
    if umi_location == "umi5" or umi_location == "umidouble":
        intPositionForward = re.search(forward_anchor, read).start()
        intPositionUmiForward = intPositionForward - int(umi_length)
        strUmiForward = read[intPositionUmiForward:intPositionForward]
        if umi_location == "umi5":
            tplCheckReverse = re.search(reverse_anchor, read)
            if tplCheckReverse is not None:
                return strUmiForward
            else:
                pass
        elif umi_location == "umidouble":
            intPositionReverse = re.search(reverse_anchor, read).end()
            intPositionUmiReverse = intPositionReverse + int(umi_length)
            strUmiReverse = read[intPositionReverse:intPositionUmiReverse]
            return strUmiForward, strUmiReverse
        else:
            pass
    elif umi_location == "umi3":
        tplCheckForward = re.search(forward_anchor, read)
        if tplCheckForward is not None:
            intPositionReverse = re.search(reverse_anchor, read).end()
            intPositionUmiReverse = intPositionReverse + int(umi_length)
            strUmiReverse = read[intPositionReverse:intPositionUmiReverse]
            return strUmiReverse
        else:
            pass
    else:
        pass


# The getRegex function.
# This function creates a regex string using a nucleotide string as input. This
# regex string is based on the IUPAC ambiguity codes. The function loops
# through a list version of the nucleotide string and checks per character if
# it is a ambiguous character. If a ambiguous character is found, it is
# replaced by a regex version. The function returns the new regex string.
def getRegex(strLine):
    dicAmbiguityCodes = {
        "M": "[AC]",
        "R": "[AG]",
        "W": "[AT]",
        "S": "[CG]",
        "Y": "[CT]",
        "K": "[GT]",
        "V": "[ACG]",
        "H": "[ACT]",
        "D": "[AGT]",
        "B": "[CGT]",
        "N": "[GATC]",
    }
    lstLine = list(strLine)
    for intPosition in range(len(lstLine)):
        if (
            lstLine[intPosition] != "A"
            and lstLine[intPosition] != "T"
            and lstLine[intPosition] != "G"
            and lstLine[intPosition] != "C"
        ):
            lstLine[intPosition] = dicAmbiguityCodes[lstLine[intPosition]]
        else:
            pass
    return "".join(lstLine)


# The getUmi function.
# This function controls the UMI searching approach. It first uses the
# functions getRegex to create regex strings of both the forward and reverse
# primers/adapters. The regex strings are then directed to the associated
# anchor functions [primer/adapter/zero].
def getUmi(
    umi_location, anchor_type, umi_length, forward_anchor, reverse_anchor, read
):
    regex_forward_anchor = getRegex(forward_anchor)
    reverse_anchorComplement = Seq(reverse_anchor)
    reverse_anchorComplement = reverse_anchorComplement.reverse_complement()
    strRegexReverseComplement = getRegex(reverse_anchorComplement)
    if anchor_type == "primer":
        try:
            return usePrimer(
                umi_location,
                umi_length,
                read,
                regex_forward_anchor,
                strRegexReverseComplement,
            )
        except AttributeError:
            pass
    elif anchor_type == "adapter":
        try:
            return useAdapter(
                umi_location,
                umi_length,
                read,
                regex_forward_anchor,
                strRegexReverseComplement,
            )
        except AttributeError:
            pass
    elif anchor_type == "zero":
        try:
            return useZeroPosition(
                umi_location,
                umi_length,
                read,
                regex_forward_anchor,
                strRegexReverseComplement,
            )
        except AttributeError:
            pass
    else:
        pass


def find_lowest_hamming_distance(strSearchSequence, read, lstAllVariants):
    """
    The find_lowest_hamming_distance function:
    """
    dicLowestVariants = {}
    dicLowestVariantsPostions = {}
    intLastCharacter = len(strSearchSequence)
    for intPosition in range(len(read)):
        strSubSequence = read[intPosition:intLastCharacter]
        if len(strSubSequence) == len(strSearchSequence):
            if strSubSequence.startswith(strSearchSequence[:1]) and strSubSequence.endswith(strSearchSequence[-1:]):
                dicVariantCheck = {}
                for strVariant in lstAllVariants:
                    dicVariantCheck[strVariant] = cjellyfish.hamming_distance(strVariant, strSubSequence)
                strLowestKey = min(dicVariantCheck, key=dicVariantCheck.get)
                dicLowestVariants[strLowestKey] = dicVariantCheck[strLowestKey]
                dicLowestVariantsPostions[strLowestKey] = [intPosition, intLastCharacter]
        intLastCharacter += 1
    strLowestVaraintPosition = dicLowestVariantsPostions[min(dicLowestVariants, key=dicLowestVariants.get)]
    return strLowestVaraintPosition
    


def retrieve_umi_code(
    umi_location, anchor_type, umi_length, forward_anchor, reverse_anchor, read
):
    """
    The retrieve_umi_code function:
    """
    regex_forward_anchor = getRegex(forward_anchor)
    reverse_anchor_complement = Seq(reverse_anchor)
    reverse_anchor_complement = reverse_anchor_complement.complement()
    regex_reverse_anchor_complement = getRegex(reverse_anchor_complement)
    all_regex_variants_forward_anchor = list(sre_yield.AllStrings(regex_forward_anchor))
    all_regex_variants_reverse_anchor = list(sre_yield.AllStrings(regex_reverse_anchor_complement))
    if umi_location == "umi5":
        anchor_positions_list = find_lowest_hamming_distance(forward_anchor, read, all_regex_variants_forward_anchor)
        if anchor_type == "primer":
            intStart = int(anchor_positions_list[0]) - umi_length
            strUmi = read[intStart:anchor_positions_list[0]]
        elif anchor_type == "adapter":
            intEnd = int(anchor_positions_list[1]) + umi_length
            strUmi = read[anchor_positions_list[1]:intEnd]
        elif anchor_type == "zero":
            pass
    elif umi_location == "umi3":
        anchor_positions_list = find_lowest_hamming_distance(regex_reverse_anchor_complement, read[::-1], all_regex_variants_reverse_anchor)
        if anchor_type == "primer":
            intStart = int(anchor_positions_list[0]) - umi_length
            strUmi = read[intStart:anchor_positions_list[0]]
        elif anchor_type == "adapter":
            intEnd = int(anchor_positions_list[1]) + umi_length
            strUmi = read[anchor_positions_list[1]:intEnd]
        elif anchor_type == "zero":
            pass
    elif umi_location == "umidouble":
        pass
    return strUmi


# The processInputFile function.
# This function calls the retrieve_umi_code function for the supplied read, this outputs
# one or two UMI codes. In the case of a double UMI [umidouble], the two UMIs
# are combined. The length of the UMI is checked before continuing. The
# getFastaFile function is called if the read contains a UMI.
def processInputFile(
    strHeader,
    read,
    umi_location,
    anchor_type,
    umi_length,
    forward_anchor,
    reverse_anchor,
    temporary_directory,
    unique_umi_dictionary,
    unique_umi_count,
):
    try:
        strUmi = retrieve_umi_code(
            umi_location,
            anchor_type,
            umi_length,
            forward_anchor.upper(),
            reverse_anchor.upper(),
            read,
        )
    except UnboundLocalError:
        pass
    #print(strUmi)


"""
    try:
        if strUmi is not None:
            if umi_location == "umi5" or umi_location == "umi3":
                intLengthPotentialUmi = len(strUmi)
                if int(intLengthPotentialUmi) == int(umi_length):
                    strCode = strUmi
                else:
                    strCode = None
            elif umi_location == "umidouble":
                strCombined = strUmi[0] + strUmi[1]
                intLengthPotentialUmi = len(strCombined)
                intDoubleUmi = umi_length * 2
                if int(intLengthPotentialUmi) == int(intDoubleUmi):
                    strCode = strCombined
                else:
                    strCode = None
            else:
                pass
        else:
            pass
    except UnboundLocalError:
        pass
    try:
        if strCode is not None:
            if strCode not in unique_umi_dictionary:
                unique_umi_dictionary[strCode] = unique_umi_count
                unique_umi_count += 1
            else:
                pass
        else:
            pass
    except UnboundLocalError:
        pass
    try:
        if strCode is not None:
            getFastaFile(
                temporary_directory, unique_umi_dictionary, strHeader, read, strCode,
            )
        else:
            pass
    except UnboundLocalError:
        pass
    strUmi = None
    strCode = None
    return unique_umi_dictionary, unique_umi_count
"""


def run_caltha(
    input_file,
    main_working_directory,
    umi_location,
    anchor_type,
    umi_length,
    forward_anchor,
    reverse_anchor,
    minimum_abundance,
    identity_percentage,
    input_format,
):
    """
    The run_caltha function:
        This function controls and calls the main functionality of Caltha. It
        loops through the input file(s) using pyfastx.
    """
    starting_time = time.time()
    input_file_name = input_file.split("/")[-1].split(".")[0]
    output_file_name = main_working_directory + "/" + input_file_name
    temporary_directory = generate_working_directories(main_working_directory, True)
    unique_umi_dictionary = {}
    unique_umi_count = 1
    if input_format == "fasta":
        for pyfastx_record in pyfastx.Fasta(input_file):
            umi_output_objects = processInputFile(
                pyfastx_record.name,
                pyfastx_record.seq.upper(),
                umi_location,
                anchor_type,
                umi_length,
                forward_anchor,
                reverse_anchor,
                temporary_directory,
                unique_umi_dictionary,
                unique_umi_count,
            )
            # unique_umi_dictionary = umi_output_objects[0]
            # unique_umi_count = umi_output_objects[1]
    elif input_format == "fastq":
        for pyfastx_record in pyfastx.Fastq(input_file):
            umi_output_objects = processInputFile(
                pyfastx_record.name,
                pyfastx_record.seq.upper(),
                umi_location,
                anchor_type,
                umi_length,
                forward_anchor,
                reverse_anchor,
                temporary_directory,
                unique_umi_dictionary,
                unique_umi_count,
            )
            # unique_umi_dictionary = umi_output_objects[0]
            # unique_umi_count = umi_output_objects[1]
    else:
        pass
    # generate_zip_archive(
    #    (temporary_directory + "/pre_validation"),
    #    (output_file_name + "_PREVALIDATION.zip"),
    #    ".fasta",
    # )
    # getDereplication(temporary_directory)
    # getSortBySize(temporary_directory, minimum_abundance)
    # setClusterSize(temporary_directory, identity_percentage)
    # setOutputFiles(temporary_directory, output_file_name)
    end_time = time.time() - starting_time
    end_message = ntpath.basename(input_file) + ": " + "%.2f" % end_time + "s"
    return end_message


def remove_working_directory(directory_path):
    """
    The remove_working_directory function:
        This function removes all temporary working directories.
    """
    shutil.rmtree(directory_path)


def generate_zip_archive(directory_path, zip_archive, file_extension):
    """
    The generate_zip_archive function:
        This function creates a zip archive from all files in the specified
        directory.
    """
    with zipfile.ZipFile(zip_archive, "w") as zip_object:
        for file_name in os.listdir(directory_path):
            if file_name.endswith(file_extension):
                full_path = directory_path + "/" + file_name
                zip_object.write(full_path, os.path.basename(full_path))


def generate_input_file_list(input_file, main_working_directory):
    """
    The generate_input_file_list function:
        This function copies the input file(s) to a new temporary folder. It
        then creates a list of all input file names.
    """
    input_file_directory = main_working_directory + "/" + "input_files"
    os.makedirs(input_file_directory)
    if (
        os.path.splitext(input_file)[1] == ".zip"
        or os.path.splitext(input_file)[1] == ".ZIP"
    ):
        with zipfile.ZipFile(input_file, "r") as zip_object:
            zip_object.extractall(input_file_directory)
            input_file_list = [
                input_file_directory + "/" + file
                for file in zipfile.ZipFile.namelist(zip_object)
            ]
    else:
        shutil.copyfile(input_file, input_file_directory + "/" + ntpath.basename(input_file))
        input_file_list = [input_file_directory + "/" + ntpath.basename(input_file)]
    return input_file_list


def generate_working_directories(temporary_directory, subdirectories):
    """
    The generate_working_directories function:
        This function creates the main temporary working directory and all
        subprocess directories. It checks if the directories already exist and
        creates them if they don't.
    """
    random_characters = "".join(
        random.choice(string.ascii_lowercase) for i in range(10)
    )
    working_directory_list = []
    working_directory_list.append(temporary_directory + "/" + random_characters)
    if subdirectories is True:
        working_directory_list.append(temporary_directory + "/" + random_characters + "/pre_validation")
        working_directory_list.append(temporary_directory + "/" + random_characters + "/vsearch_clustering")
        working_directory_list.append(temporary_directory + "/" + random_characters + "/post_vsearch_clustering")
    else:
        pass
    for temporary_directory in working_directory_list:
        if not os.path.exists(temporary_directory):
            os.makedirs(temporary_directory)
        else:
            pass
    return working_directory_list[0]


def parse_argvs():
    """
    The parse_argvs function:
    """
    description = "A python package for processing UMI tagged mixed amplicon\
                   metabarcoding data."
    epilog = "This python package requires one extra dependency which can be\
              easily installed with conda (conda install -c bioconda\
              vsearch=2.15.2)."
    parser = argparse.ArgumentParser(
        description=description,
        epilog=epilog,
        formatter_class=argparse.ArgumentDefaultsHelpFormatter,
    )
    parser.add_argument(
        "-v", "--version", action="version", version="%(prog)s [0.7]"
    )
    parser.add_argument(
        "-i",
        "--input",
        action="store",
        dest="input_file",
        type=str,
        default=argparse.SUPPRESS,
        help="The input fasta/fastq file(s). This can either be a zip archive\
              or a single fasta/fastq file, both uncompressed and gzip.",
    )
    parser.add_argument(
        "-t",
        "--tabular",
        action="store",
        dest="output_table",
        type=str,
        default=argparse.SUPPRESS,
        help="The output tabular zip file.",
    )
    parser.add_argument(
        "-z",
        "--zip",
        action="store",
        dest="output_pre_validation",
        type=str,
        default=argparse.SUPPRESS,
        help="The pre validation zip file.",
    )
    parser.add_argument(
        "-b",
        "--blast",
        action="store",
        dest="output_blast_file",
        type=str,
        default=argparse.SUPPRESS,
        help="The output blast zip file.",
    )
    parser.add_argument(
        "-f",
        "--format",
        action="store",
        dest="input_format",
        type=str,
        choices=["fasta", "fastq"],
        nargs="?",
        default="fasta",
        help="The format of the input file.",
    )
    parser.add_argument(
        "-l",
        "--location",
        action="store",
        dest="umi_location",
        type=str,
        choices=["umi5", "umi3", "umidouble"],
        nargs="?",
        default="umi5",
        help="Search for UMIs at the 5'-end, 3'-end or at the 5'-end and\
              3'-end.",
    )
    parser.add_argument(
        "-a",
        "--anchor",
        action="store",
        dest="anchor_type",
        type=str,
        choices=["primer", "adapter", "zero"],
        nargs="?",
        default="primer",
        help="Which anchor type to use.",
    )
    parser.add_argument(
        "-u",
        "--length",
        action="store",
        dest="umi_length",
        type=int,
        default="5",
        help="The length of the UMI sequence.",
    )
    parser.add_argument(
        "-y",
        "--identity",
        action="store",
        dest="identity_percentage",
        type=float,
        default="0.97",
        help="The identity percentage with which to perform the validation.",
    )
    parser.add_argument(
        "-c",
        "--abundance",
        action="store",
        dest="minimum_abundance",
        type=int,
        default="1",
        help="The minimum abundance of a sequence in order for it to be\
              included during validation.",
    )
    parser.add_argument(
        "-w",
        "--forward",
        action="store",
        dest="forward_anchor",
        type=str,
        default=argparse.SUPPRESS,
        help="The 5'-end anchor nucleotides.",
    )
    parser.add_argument(
        "-r",
        "--reverse",
        action="store",
        dest="reverse_anchor",
        type=str,
        default=argparse.SUPPRESS,
        help="The 3'-end anchor nucleotides.",
    )
    parser.add_argument(
        "-d",
        "--directory",
        action="store",
        dest="temporary_directory",
        type=str,
        default=".",
        help="The location of the temporary working directory (not created\
              by Caltha).",
    )
    parser.add_argument(
        "-@",
        "--threads",
        action="store",
        dest="threads",
        type=int,
        default=mp.cpu_count(),
        help="The number of threads to run Caltha with.",
    )
    argvs = parser.parse_args()
    return argvs


def main():
    """
    The main function:
    """
    user_arguments = parse_argvs()
    main_working_directory = generate_working_directories(user_arguments.temporary_directory, False)
    input_file_list = generate_input_file_list(user_arguments.input_file, main_working_directory)
    multi_processing_pool = mp.Pool(int(user_arguments.threads))
    multi_processing_temporary = [
        multi_processing_pool.apply_async(
            run_caltha,
            args=(
                input_file,
                main_working_directory,
                user_arguments.umi_location,
                user_arguments.anchor_type,
                user_arguments.umi_length,
                user_arguments.forward_anchor,
                user_arguments.reverse_anchor,
                user_arguments.minimum_abundance,
                user_arguments.identity_percentage,
                user_arguments.input_format,
            ),
        )
        for input_file in input_file_list
    ]
    multi_processing_pool.close()
    multi_processing_pool.join()
    multi_processing_output = [mpProcess.get() for mpProcess in multi_processing_temporary]
    # generate_zip_archive(main_working_directory, user_arguments.output_table, ".tbl")
    # generate_zip_archive(main_working_directory, user_arguments.output_pre_validation, ".zip")
    # generate_zip_archive(main_working_directory, user_arguments.output_blast_file, ".fasta")
    # remove_working_directory(main_working_directory)


if __name__ == "__main__":
    main()

"""
Additional information:
"""
